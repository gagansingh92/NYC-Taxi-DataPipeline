{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c8e63f",
   "metadata": {},
   "source": [
    "# ETL\n",
    "\n",
    "                                                                              Team 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd9ebfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8_/xl0x5_ys68z81lttj5qwwg6h0000gn/T/ipykernel_65167/2880670634.py:3: DeprecationWarning: `import pandas_profiling` is going to be deprecated by April 1st. Please use `import ydata_profiling` instead.\n",
      "  import pandas_profiling  # Importing the pandas_profiling library for generating data profiling reports\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd  # Importing the pandas library for data manipulation and analysis\n",
    "import pandas_profiling  # Importing the pandas_profiling library for generating data profiling reports\n",
    "import requests  # Importing the requests library for making HTTP requests\n",
    "import boto3  # Importing the boto3 library for interacting with AWS services\n",
    "from sqlalchemy import create_engine # Importing the create_engine function from the sqlalchemy library for database interaction\n",
    "import configparser  # Importing the configparser library for reading configuration files\n",
    "import warnings  # Importing the warnings module for handling warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warning messages\n",
    "from botocore.exceptions import NoCredentialsError  # Importing the NoCredentialsError exception from the botocore library\n",
    "import json # Importing jason library to ready the jason file\n",
    "from io import StringIO\n",
    "\n",
    "# load file from S3:datalz2 file: Yellow_Taxi_Trip_Data\n",
    "# Define your bucket names, file patterns, and AWS profile name\n",
    "taxi_bucket_name = 'datalz2'\n",
    "zone_bucket_name = 'datalz1'\n",
    "taxi_file_pattern = 'Yellow_Taxi_Trip_Data'\n",
    "zone_file_pattern = 'taxi_zones'\n",
    "aws_profile_name = 'datapro'\n",
    "\n",
    "# Initialize a session using your profile\n",
    "session = boto3.Session(profile_name=aws_profile_name)\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# Getting taxi data file list and reading the first file\n",
    "response_taxi = s3.list_objects_v2(Bucket=taxi_bucket_name, Prefix=taxi_file_pattern)\n",
    "if 'Contents' in response_taxi:\n",
    "    for file in response_taxi['Contents']:\n",
    "        file_name = file['Key']\n",
    "        if taxi_file_pattern in file_name:\n",
    "            obj = s3.get_object(Bucket=taxi_bucket_name, Key=file_name)\n",
    "            dfs = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))\n",
    "            break\n",
    "\n",
    "# Getting zone data file list and reading the first file\n",
    "response_zone = s3.list_objects_v2(Bucket=zone_bucket_name, Prefix=zone_file_pattern)\n",
    "if 'Contents' in response_zone:\n",
    "    for file in response_zone['Contents']:\n",
    "        file_name = file['Key']\n",
    "        if zone_file_pattern in file_name:\n",
    "            obj = s3.get_object(Bucket=zone_bucket_name, Key=file_name)\n",
    "            tzdfs = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))\n",
    "            break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a26102ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-------##---------##--------##--------##---------##---------##-----------##\n",
    "\n",
    "# Droping Duplicates\n",
    "dfs = dfs.drop_duplicates().reset_index(drop=True)\n",
    "# Droping Na values\n",
    "dfs = dfs.dropna(how='any')\n",
    "\n",
    "# Standarizing the values as per the data dictionary\n",
    "\t# In Passenger count should not be grater than 9\n",
    "\t# In RatecodeID should not be grater than 6\n",
    "\t# In payment_type should not be grater than 6\n",
    "\n",
    "dfs = dfs.loc[(dfs['ratecodeid'] <= 6) & (dfs['passenger_count'] <= 9) & (dfs['payment_type'] <= 6)]\n",
    "\n",
    "# Convert all column names to lowercase and replace spaces with underscores\n",
    "dfs.columns = dfs.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "# Rename specific columns\n",
    "dfs = dfs.rename(columns={\n",
    "    'vendorid': 'vendor_id',\n",
    "    'ratecodeid': 'rate_code_id',\n",
    "    'pulocationid': 'pu_location_id',\n",
    "    'dolocationid': 'do_location_id',\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c52c5040",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-------##---------##--------##--------##---------##---------##-----------##\n",
    "\n",
    "# Standardize the columns dtypes\n",
    "\t# pickup_datetime and dropoff_datetime were both of type object.\n",
    "\t# convert it to datetime object\n",
    "\n",
    "dfs['tpep_pickup_datetime']=pd.to_datetime(dfs['tpep_pickup_datetime'])\n",
    "dfs['tpep_dropoff_datetime']=pd.to_datetime(dfs['tpep_dropoff_datetime'])\n",
    "\n",
    "# Fill NaN values with a specific value\n",
    "dfs['vendor_id'].fillna(-1, inplace=True)\n",
    "dfs['passenger_count'].fillna(-1, inplace=True)\n",
    "dfs['rate_code_id'].fillna(-1, inplace=True)\n",
    "dfs['payment_type'].fillna(-1, inplace=True)\n",
    "dfs['pu_location_id'].fillna(-1, inplace=True)\n",
    "dfs['do_location_id'].fillna(-1, inplace=True)\n",
    "\n",
    "# Convert the columns to integer\n",
    "dfs['vendor_id'] = dfs['vendor_id'].astype(int)\n",
    "dfs['passenger_count'] = dfs['passenger_count'].astype(int)\n",
    "dfs['rate_code_id'] = dfs['rate_code_id'].astype(int)\n",
    "dfs['payment_type'] = dfs['payment_type'].astype(int)\n",
    "dfs['pu_location_id'] = dfs['pu_location_id'].astype(int)\n",
    "dfs['do_location_id'] = dfs['do_location_id'].astype(int)\n",
    "\n",
    "##-------##---------##--------##--------##---------##---------##-----------##\n",
    "\n",
    "# Defining RDS MySQL details and creating engine\n",
    "\n",
    "# using json file to get the credentials of MySql\n",
    "mysql_username = 'taxirpt' #MYSQL_USERNAME\n",
    "mysql_password = 'taxiadmin!1' #MYSQL_PASSWORD\n",
    "\n",
    "# Defining RDS Hostname, Port and Schema\n",
    "mysql_host = 'taxidw.cabnnjliqhn5.us-east-1.rds.amazonaws.com'\n",
    "mysql_port = '3306'\n",
    "mysql_database = 'taxi_dw'  # Schema \n",
    "\n",
    "\n",
    "# Creating the db engine\n",
    "engine = create_engine(f'mysql+pymysql://{mysql_username}:{mysql_password}@{mysql_host}:{mysql_port}/{mysql_database}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7074df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-------##---------##--------##--------##---------##---------##-----------##\n",
    "\n",
    "# Creating Dimensional data tables\n",
    "\n",
    "#Date Time Dim table\n",
    "\t#- dropoff_datetime_dim\n",
    "\t#- pickup_datetime_dim\n",
    "\n",
    "# Creating new dataset fro datetime dimension. \n",
    "pickup_datetime_dim = dfs[['tpep_pickup_datetime']].drop_duplicates().copy().reset_index(drop=True)\n",
    "dropoff_datetime_dim = dfs[['tpep_dropoff_datetime']].drop_duplicates().copy().reset_index(drop=True)\n",
    "\n",
    "# tpep_pickup_datetime\n",
    "pickup_datetime_dim['tpep_pickup_datetime'] = pickup_datetime_dim['tpep_pickup_datetime']\n",
    "pickup_datetime_dim['pickup_hour'] = pickup_datetime_dim['tpep_pickup_datetime'].dt.hour\n",
    "pickup_datetime_dim['pickup_day'] = pickup_datetime_dim['tpep_pickup_datetime'].dt.day_name()\n",
    "pickup_datetime_dim['pickup_month'] = pickup_datetime_dim['tpep_pickup_datetime'].dt.month\n",
    "pickup_datetime_dim['pickup_year'] = pickup_datetime_dim['tpep_pickup_datetime'].dt.year\n",
    "pickup_datetime_dim['pickup_weekday'] = pickup_datetime_dim['tpep_pickup_datetime'].dt.weekday\n",
    "\n",
    "# tpep_dropoff_datetime\n",
    "dropoff_datetime_dim['tpep_dropoff_datetime'] = dropoff_datetime_dim['tpep_dropoff_datetime']\n",
    "dropoff_datetime_dim['dropoff_hour'] = dropoff_datetime_dim['tpep_dropoff_datetime'].dt.hour\n",
    "dropoff_datetime_dim['dropoff_day'] = dropoff_datetime_dim['tpep_dropoff_datetime'].dt.day_name()\n",
    "dropoff_datetime_dim['dropoff_month'] = dropoff_datetime_dim['tpep_dropoff_datetime'].dt.month\n",
    "dropoff_datetime_dim['dropoff_year'] = dropoff_datetime_dim['tpep_dropoff_datetime'].dt.year\n",
    "dropoff_datetime_dim['dropoff_weekday'] = dropoff_datetime_dim['tpep_dropoff_datetime'].dt.weekday\n",
    "\n",
    "pickup_datetime_dim['pu_datetime_id'] = pickup_datetime_dim.index + 1\n",
    "dropoff_datetime_dim['do_datetime_id'] = dropoff_datetime_dim.index + 1\n",
    "\n",
    "##-------##---------##--------##--------##---------##---------##-----------##\n",
    "\n",
    "# defining a function that lets us determine what time of the day the ride was taken. \n",
    "# Creates 4 time zones ‘Morning’ (from 6:00 am to 11:59 pm), \n",
    "# ‘Afternoon’ (from 12 noon to 3:59 pm), ‘Evening’ (from 4:00 pm to 9:59 pm),\n",
    "# ‘Late Night’ (from 10:00 pm to 5:59 am)\n",
    "\n",
    "def time_of_day(x):\n",
    "    if x in range(6,12):\n",
    "        return 'Morning'\n",
    "    elif x in range(12,16):\n",
    "        return 'Afternoon'\n",
    "    elif x in range(16,22):\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Late night'\n",
    "\n",
    "##-------##---------##--------##--------##---------##---------##-----------##\n",
    "\n",
    "# Now let us apply this function and create new columns in the dataset.\n",
    "\n",
    "pickup_datetime_dim['pickup_timeofday']=pickup_datetime_dim['pickup_hour'].apply(time_of_day)\n",
    "dropoff_datetime_dim['dropoff_timeofday']=dropoff_datetime_dim['dropoff_hour'].apply(time_of_day)\n",
    "\n",
    "# datetime_dim = datetime_dim.rename(columns={'tpep_pickup_datetime': 'datetime_id'}).reset_index(drop=True)\n",
    "pickup_datetime_dim = pickup_datetime_dim[['pu_datetime_id', 'tpep_pickup_datetime', \\\n",
    "                                           'pickup_hour', 'pickup_day', \\\n",
    "                                           'pickup_month', 'pickup_year', \\\n",
    "                                           'pickup_weekday', 'pickup_timeofday',]]\n",
    "# datetime_dim = datetime_dim.rename(columns={'tpep_pickup_datetime': 'datetime_id'}).reset_index(drop=True)\n",
    "dropoff_datetime_dim = dropoff_datetime_dim[['do_datetime_id','tpep_dropoff_datetime', \\\n",
    "                                             'dropoff_hour', 'dropoff_day', \\\n",
    "                                             'dropoff_month', 'dropoff_year', \\\n",
    "                                             'dropoff_weekday', 'dropoff_timeofday']]\n",
    "\n",
    "# Coverting tpep_pickup_datetime to object dtype\n",
    "pickup_datetime_dim['tpep_pickup_datetime'] = pickup_datetime_dim['tpep_pickup_datetime'].astype(str)\n",
    "# Coverting tpep_pickup_datetime to object dtype\n",
    "dropoff_datetime_dim['tpep_dropoff_datetime'] = dropoff_datetime_dim['tpep_dropoff_datetime'].astype(str)\n",
    "\n",
    "# Loading pickup_datetime_dim data to RDS\n",
    "\n",
    "# Select columns 'pickup_datetime_dim' from the DataFrame\n",
    "pu_dt_dim = pickup_datetime_dim[['tpep_pickup_datetime', 'pickup_hour', 'pickup_day', \\\n",
    "'pickup_month', 'pickup_year', 'pickup_weekday', 'pickup_timeofday' ]]\n",
    "\n",
    "# Loading data\n",
    "pu_dt_dim.to_sql('pickup_datetime_dim', con=engine, if_exists='append', index=False)\n",
    "\n",
    "# Loading dropoff_datetime_dim data to RDS \n",
    "\n",
    "# Select columns 'pickup_datetime_dim' from the DataFrame\n",
    "do_dt_dim = dropoff_datetime_dim[['tpep_dropoff_datetime', 'dropoff_hour', 'dropoff_day', \\\n",
    "'dropoff_month', 'dropoff_year', 'dropoff_weekday', 'dropoff_timeofday' ]]\n",
    "\n",
    "# Loading data\n",
    "do_dt_dim.to_sql('dropoff_datetime_dim', con=engine, if_exists='append', index=False)\n",
    "\n",
    "## Retriving Data from local rds, table: rate_code_dim\n",
    "\n",
    "# specifying SQL query\n",
    "sql_query1 = \"SELECT * FROM pickup_datetime_dim\"\n",
    "\n",
    "# executing the query and assign the result to a pandas DataFrame\n",
    "pudt_dim = pd.read_sql_query(sql_query1, engine)\n",
    "\n",
    "# Conveting tpep_pickup_datetime to datetime format\n",
    "pudt_dim['tpep_pickup_datetime'] = pd.to_datetime(pudt_dim['tpep_pickup_datetime'])\n",
    "\n",
    "## Retriving Data from local rds, table: rate_code_dim\n",
    "\n",
    "# specifying SQL query\n",
    "sql_query2 = \"SELECT * FROM dropoff_datetime_dim\"\n",
    "\n",
    "# executing the query and assign the result to a pandas DataFrame\n",
    "dodt_dim = pd.read_sql_query(sql_query2, engine)\n",
    "\n",
    "# Conveting tpep_dropoff_datetime to datetime format\n",
    "dodt_dim['tpep_dropoff_datetime'] = pd.to_datetime(dodt_dim['tpep_dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a448e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-------##---------##--------##--------##---------##---------##-----------##\n",
    "\n",
    "#rate_code_dim\n",
    "\n",
    "## Creating rate_code_dim Dimension: \n",
    "\n",
    "# deriving from dfs\n",
    "\n",
    "rate_code_type = {\n",
    "    1: \"Standard rate\",\n",
    "    2: \"JFK\",\n",
    "    3: \"Newark\",\n",
    "    4: \"Nassau or Westchester\",\n",
    "    5: \"Negotiated fare\",\n",
    "    6: \"Group ride\"\n",
    "}\n",
    "\n",
    "rate_code_dim = dfs[['rate_code_id']].drop_duplicates().copy().reset_index(drop=True)\n",
    "rate_code_dim['rc_id'] = rate_code_dim.index + 1 # Add 1 to the index\n",
    "rate_code_dim['rate_code_name'] = rate_code_dim['rate_code_id'].map(rate_code_type)\n",
    "rate_code_dim = rate_code_dim[['rc_id', 'rate_code_id', 'rate_code_name']]\n",
    "\n",
    "# Hard Coded\n",
    "# define the rate code types\n",
    "rate_code_type = {\n",
    "    1: \"Standard rate\",\n",
    "    2: \"JFK\",\n",
    "    3: \"Newark\",\n",
    "    4: \"Nassau or Westchester\",\n",
    "    5: \"Negotiated fare\",\n",
    "    6: \"Group ride\"\n",
    "}\n",
    "\n",
    "# create a DataFrame from the dictionary\n",
    "rate_code_dim = pd.DataFrame(list(rate_code_type.items()), columns=['rate_code_id', 'rate_code_name'])\n",
    "\n",
    "# add the rc_id column\n",
    "rate_code_dim['rc_id'] = rate_code_dim.index + 1\n",
    "\n",
    "# rearrange the columns\n",
    "rate_code_dim = rate_code_dim[['rc_id', 'rate_code_id', 'rate_code_name']]\n",
    "\n",
    "# Loading data to RDS\n",
    "\n",
    "# Select columns 'rate_code_id' and 'rate_code_name' from the DataFrame\n",
    "df_selected = rate_code_dim[['rate_code_id', 'rate_code_name']]\n",
    "\n",
    "df_selected.to_sql('rate_code_dim', con=engine, if_exists='append', index=False)\n",
    "\n",
    "## Retriving Data from local rds, table: rate_code_dim\n",
    "\n",
    "# specifying SQL query\n",
    "sql_query3 = \"SELECT * FROM rate_code_dim\"\n",
    "\n",
    "# executing the query and assign the result to a pandas DataFrame\n",
    "rc_dim = pd.read_sql_query(sql_query3, engine)\n",
    "\n",
    "rc_dim['rate_code_id'] = rc_dim['rate_code_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b004b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-------##---------##--------##--------##---------##---------##-----------##\n",
    "\n",
    "##passenger_count_dim\n",
    "\n",
    "passenger_count_dim = dfs[['passenger_count']].drop_duplicates().copy().reset_index(drop=True)\n",
    "\n",
    "passenger_count_dim['passenger_count_id'] = passenger_count_dim.index + 1\n",
    "\n",
    "passenger_count_dim = passenger_count_dim[['passenger_count_id','passenger_count']]\n",
    "\n",
    "# Loading data to RDS,\n",
    "\n",
    "# Select columns 'passenger_count' from the DataFrame\n",
    "pass_count_dim = passenger_count_dim[['passenger_count']]\n",
    "\n",
    "# Loading data\n",
    "pass_count_dim.to_sql('passenger_count_dim', con=engine, if_exists='append', index=False)\n",
    "\n",
    "## Retriving Data from local rds, table: rate_code_dim\n",
    "\n",
    "# specifying SQL query\n",
    "sql_query4 = \"SELECT * FROM passenger_count_dim\"\n",
    "\n",
    "# executing the query and assign the result to a pandas DataFrame\n",
    "pc_dim = pd.read_sql_query(sql_query4, engine)\n",
    "\n",
    "pc_dim['passenger_count'] = pc_dim['passenger_count'].astype(int)\n",
    "\n",
    "##-------##---------##--------##--------##---------##---------##-----------##\n",
    "\n",
    "#vendor_dim\n",
    "\n",
    "# driving from dfs dataframe\n",
    "\n",
    "vendor_name = {\n",
    "    1:\"Creative Mobile Technologies, LLC\",\n",
    "    2:\"Curb Mobility - formerly VeriFone Inc\"\n",
    "}\n",
    "vendor_dim = dfs[['vendor_id']].drop_duplicates().copy().reset_index(drop=True)\n",
    "vendor_dim['vc_id'] = vendor_dim.index + 1 # Add 1 to the index\n",
    "vendor_dim['vendor_name'] = vendor_dim['vendor_id'].map(vendor_name)\n",
    "vendor_dim = vendor_dim[['vc_id', 'vendor_id', 'vendor_name']]\n",
    "\n",
    "# hardcoded \n",
    "\n",
    "# define the vendor names\n",
    "vendor_name = {\n",
    "    1: \"Creative Mobile Technologies, LLC\",\n",
    "    2: \"Curb Mobility - formerly VeriFone Inc\",\n",
    "}\n",
    "\n",
    "# create a DataFrame from the dictionary\n",
    "vendor_dim = pd.DataFrame(list(vendor_name.items()), columns=['vendor_id', 'vendor_name'])\n",
    "\n",
    "# add the vc_id column\n",
    "vendor_dim['vc_id'] = vendor_dim.index + 1\n",
    "\n",
    "# rearrange the columns\n",
    "vendor_dim = vendor_dim[['vc_id', 'vendor_id', 'vendor_name']]\n",
    "\n",
    "# Loading data to RDS\n",
    "\n",
    "# Select columns 'passenger_count' from the DataFrame\n",
    "van_dim = vendor_dim[['vendor_id', 'vendor_name']]\n",
    "\n",
    "# Loading data\n",
    "van_dim.to_sql('vendor_dim', con=engine, if_exists='append', index=False)\n",
    "\n",
    "\n",
    "## Retriving Data from rds, table: rate_code_dim\n",
    "\n",
    "# specifying SQL query\n",
    "sql_query5 = \"SELECT * FROM vendor_dim\"\n",
    "\n",
    "# executing the query and assign the result to a pandas DataFrame\n",
    "v_dim = pd.read_sql_query(sql_query5, engine)\n",
    "\n",
    "v_dim['vendor_id'] = v_dim['vendor_id'].astype(int)\n",
    "\n",
    "##-------##---------##--------##--------##---------##---------##-----------##\n",
    "\n",
    "##payment_type_dim\n",
    "\n",
    "# hardcoded\n",
    "\n",
    "payment_type_name = {\n",
    "    1: \"Credit card\",\n",
    "    2: \"Cash\",\n",
    "    3: \"No charge\",\n",
    "    4: \"Dispute\",\n",
    "    5: \"Unknown\",\n",
    "    6: \"Voided trip\"\n",
    "}\n",
    "\n",
    "# Convert the dictionary into a list of tuples\n",
    "payment_type_list = [(k, v) for k, v in payment_type_name.items()]\n",
    "\n",
    "# Convert the list of tuples into a DataFrame\n",
    "payment_type_dim = pd.DataFrame(payment_type_list, columns=['payment_type_id', 'payment_type_name'])\n",
    "\n",
    "# Adding a new column 'payment_type'\n",
    "payment_type_dim['payment_type'] = payment_type_dim['payment_type_id'].astype('category')\n",
    "\n",
    "payment_type_dim = payment_type_dim[['payment_type_id', 'payment_type', 'payment_type_name']]\n",
    "\n",
    "# Loading data to RDS\n",
    "\n",
    "# Select columns 'passenger_count' from the DataFrame\n",
    "payment_dim = payment_type_dim[['payment_type','payment_type_name']]\n",
    "\n",
    "# Loading data\n",
    "payment_dim.to_sql('payment_type_dim', con=engine, if_exists='append', index=False)\n",
    "\n",
    "## Retriving Data from local rds, table: payment_dim\n",
    "\n",
    "# specifying SQL query\n",
    "sql_query6 = \"SELECT * FROM payment_type_dim\"\n",
    "\n",
    "# executing the query and assign the result to a pandas DataFrame\n",
    "pay_dim = pd.read_sql_query(sql_query6, engine)\n",
    "\n",
    "pay_dim['payment_type'] = pay_dim['payment_type'].astype(int)\n",
    "\n",
    "##-------##---------##--------##--------##---------##---------##-----------##\n",
    "\n",
    "##store_and_fwd_flag_dim\n",
    "\n",
    "# hardcode\n",
    "\n",
    "store_and_fwd_flag_name = {\n",
    "    1: \"Y\",\n",
    "    2: \"N\"\n",
    "}\n",
    "\n",
    "# Convert the dictionary into a list of tuples\n",
    "store_and_fwd_flag_list = [(k, v) for k, v in store_and_fwd_flag_name.items()]\n",
    "\n",
    "# Convert the list of tuples into a DataFrame\n",
    "store_and_fwd_flag_dim = pd.DataFrame(store_and_fwd_flag_list, columns=['store_and_fwd_flag_id', 'store_and_fwd_flag'])\n",
    "\n",
    "# Adding a new column 'store_and_fwd_flag_name'\n",
    "store_and_fwd_flag_dim['store_and_fwd_flag_name'] = store_and_fwd_flag_dim['store_and_fwd_flag'].map({'Y': 'store and forward trip', 'N': 'not a store and forward trip'})\n",
    "\n",
    "# Loading data to RDS\n",
    "\n",
    "# Select columns 'store_and_fwd_flag_dim' from the DataFrame\n",
    "store_dim = store_and_fwd_flag_dim[['store_and_fwd_flag','store_and_fwd_flag_name']]\n",
    "\n",
    "# Loading data\n",
    "store_dim.to_sql('store_and_fwd_flag_dim', con=engine, if_exists='append', index=False)\n",
    "\n",
    "## Retriving Data from local rds, table: payment_dim\n",
    "\n",
    "# specifying SQL query\n",
    "sql_query7 = \"SELECT * FROM store_and_fwd_flag_dim\"\n",
    "\n",
    "# executing the query and assign the result to a pandas DataFrame\n",
    "sflag_dim = pd.read_sql_query(sql_query7, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfa96ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-------##---------##--------##--------##---------##---------##-----------##\n",
    "\n",
    "##tazi_zone_dim\n",
    "\n",
    "\n",
    "taxi_zone_dim = tzdfs.rename(columns={\"OBJECTID\": \"tz_id\",\n",
    "                              \"Shape_Leng\": \"shape_leng\",\n",
    "                              \"the_geom\": \"lng_lat\",\n",
    "                               \"Shape_Area\": \"shape_area\",\n",
    "                             \"zone\": \"zone_name\",\n",
    "                              \"LocationID\": \"taxi_zone_location_id\",\n",
    "                             \"borough\": \"taxi_zone_borough_name\"})\n",
    "\n",
    "# Loading data to RDS\n",
    "\n",
    "# Select columns 'taxi_zone_dim' from the DataFrame\n",
    "taxiz_dim = taxi_zone_dim[['shape_leng','shape_area','zone_name','taxi_zone_location_id','taxi_zone_borough_name']]\n",
    "\n",
    "# Loading data\n",
    "taxiz_dim.to_sql('taxi_zone_dim', con=engine, if_exists='append', index=False)\n",
    "\n",
    "## Retriving Data from local rds, table: payment_dim\n",
    "\n",
    "# specifying SQL query\n",
    "sql_query8 = \"SELECT * FROM taxi_zone_dim\"\n",
    "\n",
    "# executing the query and assign the result to a pandas DataFrame\n",
    "tz_dim = pd.read_sql_query(sql_query8, engine)\n",
    "\n",
    "tz_dim['taxi_zone_location_id'] = tz_dim['taxi_zone_location_id'].astype(int)\n",
    "\n",
    "##-------##---------##--------##--------##---------##---------##-----------##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5816d16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "987609"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "## Fact-table mapping\n",
    "\n",
    "# Fact table Merging \n",
    "ft = dfs.merge(rc_dim, on='rate_code_id', how='left') \\\n",
    ".merge(pc_dim, on='passenger_count', how='left')\\\n",
    ".merge(v_dim, on='vendor_id', how='left')\\\n",
    ".merge(pay_dim, on='payment_type', how='left')\\\n",
    ".merge(sflag_dim, on='store_and_fwd_flag', how='left')\\\n",
    ".merge(pudt_dim, on='tpep_pickup_datetime', how='left')\\\n",
    ".merge(dodt_dim, on='tpep_dropoff_datetime', how='left')\n",
    "\n",
    "\n",
    "# Merging on 'pu_location_id' and 'taxi_zone_location_id'\n",
    "ft = ft.merge(tz_dim, left_on='pu_location_id', right_on='taxi_zone_location_id', suffixes=('', '_pu'))\n",
    "\n",
    "# Renaming 'taxi_zone_id' to 'pu_taxi_zone_loc_id'\n",
    "ft = ft.rename(columns={'taxi_zone_id': 'pu_taxi_zone_loc_id'})\n",
    "\n",
    "# Merging on 'do_location_id' and 'taxi_zone_location_id'\n",
    "ft = ft.merge(tz_dim, left_on='do_location_id', right_on='taxi_zone_location_id', suffixes=('_pu', '_do'))\n",
    "\n",
    "# Renaming 'taxi_zone_id' to 'do_taxi_zone_loc_id'\n",
    "ft = ft.rename(columns={'taxi_zone_id': 'do_taxi_zone_loc_id'})\n",
    "\n",
    "# Getting only required columns \n",
    "fts = ft[['rate_code_id','rc_id', 'passenger_count', 'passenger_count_id', 'vendor_id', \\\n",
    "  'vc_id', 'payment_type','payment_type_id', 'store_and_fwd_flag', 'store_and_fwd_flag_id',\\\n",
    "          'do_location_id','taxi_zone_location_id_do', 'pu_location_id','taxi_zone_location_id_pu',\\\n",
    "          'tpep_pickup_datetime','pu_datetime_id', 'tpep_dropoff_datetime','do_datetime_id']]\n",
    "\n",
    "##-------##---------##--------##--------##---------##---------##-----------##\n",
    "\n",
    "## Final fact table \n",
    "\n",
    "ft_final = ft[['vc_id','rc_id', 'passenger_count_id', 'payment_type_id','store_and_fwd_flag_id', 'pu_datetime_id', \\\n",
    "               'do_datetime_id','taxi_zone_location_id_do', 'taxi_zone_location_id_pu', \\\n",
    "               'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', \n",
    "'improvement_surcharge', 'total_amount', 'congestion_surcharge']]\n",
    "\n",
    "\n",
    "# Loading data Final fact table to RDS\n",
    "\n",
    "# Select columns 'taxi_zone_dim' from the DataFrame\n",
    "fft = ft_final[['vc_id', \n",
    "                'passenger_count_id', \n",
    "                'taxi_zone_location_id_pu',\n",
    "                'taxi_zone_location_id_do',\n",
    "                'payment_type_id',\n",
    "                'rc_id', \n",
    "                'store_and_fwd_flag_id', \n",
    "                'pu_datetime_id', \n",
    "                'do_datetime_id',\n",
    "                'fare_amount', \n",
    "                'extra', \n",
    "                'mta_tax', \n",
    "                'tip_amount', \n",
    "                'tolls_amount', \n",
    "                'improvement_surcharge', \n",
    "                'total_amount', \n",
    "                'congestion_surcharge']]\n",
    "\n",
    "# Loading data\n",
    "fft.to_sql('fact_table', con=engine, if_exists='append', index=False)\n",
    "\n",
    "##-------##---------##--------##--------##---------##---------##-----------##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f1d072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
